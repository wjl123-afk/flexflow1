{
  "paths": {
    "result_json": "/data/log/gpu_model_vllm_test/RTX_4090_24GB_PCIe/Qwen2_5-32B-Instruct/results/Qwen2_5-32B-Instruct__RTX_4090_24GB_PCIe__TP4_prompt2048_output2048__result.json",
    "server_log": "/data/log/gpu_model_vllm_test/RTX_4090_24GB_PCIe/Qwen2_5-32B-Instruct/logs/Qwen2_5-32B-Instruct__RTX_4090_24GB_PCIe__TP4_prompt2048_output2048__server.log",
    "client_log": "/data/log/gpu_model_vllm_test/RTX_4090_24GB_PCIe/Qwen2_5-32B-Instruct/logs/Qwen2_5-32B-Instruct__RTX_4090_24GB_PCIe__TP4_prompt2048_output2048__client.log"
  },
  "server_command": [
    "python",
    "-m",
    "vllm.entrypoints.openai.api_server",
    "--model",
    "/workspace/models/qwen2.5-32b/qwen2.5-32b",
    "--dtype",
    "bfloat16",
    "--tensor-parallel-size",
    "4",
    "--pipeline-parallel-size",
    "1",
    "--kv-cache-dtype",
    "fp8",
    "--max-num-seqs",
    "15",
    "--max-model-len",
    "4096",
    "--max-num-batched-tokens",
    "231",
    "--gpu-memory-utilization",
    "0.85",
    "--enable-prefix-caching",
    "--block-size",
    "16",
    "--enable-chunked-prefill",
    "--swap-space",
    "0",
    "--host",
    "0.0.0.0",
    "--port",
    "8003"
  ],
  "client_command": [
    "python",
    "-m",
    "vllm.benchmark.benchmark_serving",
    "--endpoint",
    "http://127.0.0.1:8003/v1",
    "--dataset",
    "synthetic",
    "--concurrency",
    "12",
    "--prompt-length",
    "2048",
    "--output-length",
    "2048",
    "--num-prompts",
    "1000",
    "--temperature",
    "0.0",
    "--top-p",
    "1.0",
    "--top-k",
    "0"
  ]
}