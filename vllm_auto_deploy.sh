#!/bin/bash

################################################################################
# vLLM å®Œæ•´ç¯å¢ƒè‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬
# åŠŸèƒ½ï¼šåœ¨æ–°çš„ CUDA 12.4 æœºå™¨ä¸Šä¸€é”®éƒ¨ç½² vLLM æ¨ç†ç¯å¢ƒ
# ä½œè€…ï¼šBlockElite ç ”å‘å›¢é˜Ÿï¼ˆæ”¹ç¼–ç‰ˆï¼‰
# æ—¥æœŸï¼š2025.11.19
################################################################################

set -euo pipefail

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# æ—¥å¿—å‡½æ•°
log_info()    { echo -e "${BLUE}[INFO]${NC}    $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error()   { echo -e "${RED}[ERROR]${NC}   $1"; }
print_sep()   { echo -e "${BLUE}===================================================================${NC}"; }

# å…¨å±€å˜é‡
VLLM_IMAGE="${VLLM_IMAGE:-nvidia/cuda:12.4.0-devel-ubuntu22.04}"
CONTAINER_NAME="${CONTAINER_NAME:-vllm-gpu}"
WORKSPACE_DIR="${WORKSPACE_DIR:-/workspace}"
SCRIPT_DIR="$WORKSPACE_DIR/scripts"
HOST_HTTP_PROXY="${http_proxy:-}"
HOST_HTTPS_PROXY="${https_proxy:-}"

################################################################################
# ç¬¬0æ­¥ï¼šç¯å¢ƒæ£€æŸ¥
################################################################################
check_environment() {
    print_sep
    log_info "ç¬¬0æ­¥ï¼šç¯å¢ƒæ£€æŸ¥ä¸ç³»ç»Ÿé…ç½®"
    print_sep

    log_info "æ£€æŸ¥æ“ä½œç³»ç»Ÿå†…æ ¸"
    uname -a

    log_info "æ£€æŸ¥ GPU ä¸é©±åŠ¨ç‰ˆæœ¬ï¼ˆéœ€ >= 550ï¼Œæ¨è 565+ï¼‰"
    if ! command -v nvidia-smi >/dev/null 2>&1; then
        log_error "æœªæ£€æµ‹åˆ° nvidia-smiï¼Œè¯·å…ˆå®‰è£… NVIDIA é©±åŠ¨ (CUDA 12.4+)"
        exit 1
    fi
    nvidia-smi

    log_info "æ£€æŸ¥å¯ç”¨ç£ç›˜ç©ºé—´"
    df -h "$WORKSPACE_DIR" || df -h

    log_info "æ£€æŸ¥ç½‘ç»œè¿é€šæ€§"
    ping -c 2 mirrors.tuna.tsinghua.edu.cn || log_warning "æ— æ³• ping é€šæ¸…åé•œåƒï¼Œå¯å¿½ç•¥"

    log_success "ç¯å¢ƒæ£€æŸ¥å®Œæˆ"
}

################################################################################
# ç¬¬1æ­¥ï¼šé…ç½® mihomo ä»£ç†ï¼ˆå¯é€‰ï¼‰
################################################################################
setup_mihomo_proxy() {
    print_sep
    log_info "ç¬¬1æ­¥ï¼šé…ç½® mihomo ä»£ç†ï¼ˆå¦‚å·²è¿è¡Œå¯è·³è¿‡ï¼‰"
    print_sep

    if pgrep -f "mihomo" >/dev/null 2>&1; then
        log_warning "æ£€æµ‹åˆ°å·²æœ‰ mihomo è¿›ç¨‹ï¼Œè·³è¿‡é‡æ–°ä¸‹è½½"
        return
    fi

    apt-get update
    apt-get install -y sshpass curl

    mkdir -p /opt/mihomo && cd /opt/mihomo
    log_info "é€šè¿‡ SFTP æ‹‰å– mihomo ä¸é…ç½®..."
    sshpass -p 'Yn783CWe' sftp -P 15022 -o StrictHostKeyChecking=no 15256911585@pan.blockelite.cn <<'EOF'
cd mihomo
get mihomo
get mihomo_config.yaml
quit
EOF

    chmod +x ./mihomo
    nohup ./mihomo -f mihomo_config.yaml >mihomo.log 2>&1 &
    sleep 3

    if curl -I --proxy http://127.0.0.1:7890 http://www.google.com >/dev/null 2>&1; then
        log_success "mihomo ä»£ç†å¯ç”¨"
    else
        log_warning "ä»£ç†è¿é€šæ€§éªŒè¯å¤±è´¥ï¼Œè¯·è‡ªè¡Œç¡®è®¤ 127.0.0.1:7890"
    fi
}

################################################################################
# ç¬¬2æ­¥ï¼šå®‰è£… Docker
################################################################################
install_docker() {
    print_sep
    log_info "ç¬¬2æ­¥ï¼šå®‰è£… Docker Engine"
    print_sep

    if command -v docker >/dev/null 2>&1; then
        log_warning "Docker å·²å­˜åœ¨ï¼Œè·³è¿‡å®‰è£…"
        docker --version
        return
    fi

    curl -fsSL https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu/gpg | \
        gpg --dearmor -o /etc/apt/keyrings/docker.gpg

    echo \
        "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu $(lsb_release -cs) stable" \
        >/etc/apt/sources.list.d/docker.list

    apt-get update
    apt-get install -y docker-ce docker-ce-cli containerd.io

    systemctl enable --now docker
    docker --version
    log_success "Docker å®‰è£…å®Œæˆ"
}

################################################################################
# ç¬¬3æ­¥ï¼šå®‰è£… NVIDIA Container Toolkit
################################################################################
install_nvidia_toolkit() {
    print_sep
    log_info "ç¬¬3æ­¥ï¼šå®‰è£… NVIDIA Container Toolkit"
    print_sep

    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
        gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

    curl -fsSL https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
        sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#' \
        >/etc/apt/sources.list.d/nvidia-container-toolkit.list

    apt-get update
    apt-get install -y nvidia-container-toolkit

    nvidia-ctk runtime configure --runtime=docker
    systemctl restart docker

    log_success "NVIDIA Container Toolkit å®‰è£…å®Œæˆ"
}

################################################################################
# ç¬¬4æ­¥ï¼šå‡†å¤‡å·¥ä½œç›®å½•
################################################################################
prepare_workspace() {
    print_sep
    log_info "ç¬¬4æ­¥ï¼šå‡†å¤‡ /workspace ç›®å½•ä¸è„šæœ¬ç›®å½•"
    print_sep

    mkdir -p "$WORKSPACE_DIR"/{models,configs,logs,scripts}
    chmod -R 777 "$WORKSPACE_DIR"

    cat >"$SCRIPT_DIR/README.txt" <<EOF
è¯¥ç›®å½•ç”± vLLM è‡ªåŠ¨éƒ¨ç½²è„šæœ¬ç”Ÿæˆï¼š
- run_vllm_server.sh  å¯åŠ¨ vLLM OpenAI å…¼å®¹æœåŠ¡
- bench_vllm_client.sh ä½¿ç”¨ vLLM benchmark å·¥å…·å‹æµ‹
é»˜è®¤ç«¯å£ï¼š8015ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ VLLM_PORT è¦†ç›–
EOF

    log_success "å·¥ä½œç›®å½•å‡†å¤‡å®Œæˆï¼š$WORKSPACE_DIR"
}

################################################################################
# ç¬¬5æ­¥ï¼šæ‹‰å– vLLM åŸºç¡€é•œåƒ
################################################################################
pull_vllm_image() {
    print_sep
    log_info "ç¬¬5æ­¥ï¼šæ‹‰å– CUDA åŸºç¡€é•œåƒ $VLLM_IMAGE"
    print_sep

    docker pull "$VLLM_IMAGE"
    log_success "é•œåƒæ‹‰å–å®Œæˆ"
}

################################################################################
# ç¬¬6æ­¥ï¼šå¯åŠ¨ vLLM å®¹å™¨
################################################################################
start_vllm_container() {
    print_sep
    log_info "ç¬¬6æ­¥ï¼šå¯åŠ¨ vLLM GPU å®¹å™¨ $CONTAINER_NAME"
    print_sep

    if docker ps -a | grep -q "$CONTAINER_NAME"; then
        log_warning "æ£€æµ‹åˆ°å†å²å®¹å™¨ï¼Œå…ˆåˆ é™¤"
        docker stop "$CONTAINER_NAME" >/dev/null 2>&1 || true
        docker rm "$CONTAINER_NAME" >/dev/null 2>&1 || true
    fi

    docker run -d --name "$CONTAINER_NAME" \
        --gpus all \
        --shm-size=32g \
        -e HF_HOME=/workspace/.cache/huggingface \
        -e http_proxy="$HOST_HTTP_PROXY" \
        -e https_proxy="$HOST_HTTPS_PROXY" \
        -p 8015:8015 \
        -v "$WORKSPACE_DIR":"$WORKSPACE_DIR" \
        "$VLLM_IMAGE" tail -f /dev/null

    sleep 3
    docker ps | grep "$CONTAINER_NAME"
    log_success "å®¹å™¨å·²å¯åŠ¨"
}

################################################################################
# ç¬¬7æ­¥ï¼šå®¹å™¨å†…å®‰è£… Python/vLLM ä¾èµ–
################################################################################
configure_vllm_env() {
    print_sep
    log_info "ç¬¬7æ­¥ï¼šåœ¨å®¹å™¨å†…å®‰è£… Python3 + vLLM ä¾èµ–"
    print_sep

    docker exec "$CONTAINER_NAME" bash -c '
set -e
apt-get update
DEBIAN_FRONTEND=noninteractive apt-get install -y python3 python3-pip python3-venv git curl vim
ln -sf /usr/bin/python3 /usr/bin/python
python3 -m venv /opt/vllm-venv
source /opt/vllm-venv/bin/activate
pip install --upgrade pip
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121
pip install "vllm>=0.4.2" "transformers>=4.39" accelerate sentencepiece datasets huggingface_hub
pip install "uvicorn>=0.23" fastapi
'

    log_success "å®¹å™¨å†…ä¾èµ–å®‰è£…å®Œæˆï¼ˆè™šæ‹Ÿç¯å¢ƒï¼š/opt/vllm-venvï¼‰"
}

################################################################################
# ç¬¬8æ­¥ï¼šç”Ÿæˆè¿è¡Œè„šæœ¬å¹¶åšçƒŸæµ‹
################################################################################
generate_helper_scripts() {
    print_sep
    log_info "ç¬¬8æ­¥ï¼šç”Ÿæˆ vLLM è¿è¡Œ/å‹æµ‹è„šæœ¬"
    print_sep

    cat >"$SCRIPT_DIR/run_vllm_server.sh" <<'EOF'
#!/bin/bash
set -euo pipefail

PORT="${VLLM_PORT:-8015}"
MODEL_PATH="${VLLM_MODEL_PATH:-/workspace/models/qwen2.5-32b/qwen2.5-32b}"
CONTAINER="${CONTAINER_NAME:-vllm-gpu}"

docker exec -d "$CONTAINER" bash -c "
source /opt/vllm-venv/bin/activate && \
python -m vllm.entrypoints.openai.api_server \
  --model \"$MODEL_PATH\" \
  --trust-remote-code \
  --tensor-parallel-size \${VLLM_TP:-1} \
  --max-num-seqs \${VLLM_MAX_CONCURRENCY:-6} \
  --port $PORT \
  --host 0.0.0.0 \
  --disable-log-requests \
  --served-model-name \${VLLM_MODEL_NAME:-$(basename "$MODEL_PATH")} \
  > /workspace/logs/vllm_server_\$(date +%Y%m%d_%H%M%S).log 2>&1 &"

echo "vLLM æœåŠ¡å·²åœ¨å®¹å™¨ $CONTAINER å†…å¯åŠ¨ï¼Œç«¯å£ $PORT"
EOF

    cat >"$SCRIPT_DIR/bench_vllm_client.sh" <<'EOF'
#!/bin/bash
set -euo pipefail

PORT="${VLLM_PORT:-8015}"
SERVER_HOST="${VLLM_HOST:-127.0.0.1}"
MODEL_NAME="${VLLM_MODEL_NAME:-qwen2.5-32b}"
CONTAINER="${CONTAINER_NAME:-vllm-gpu}"
RESULT_DIR="/workspace/logs/results"
mkdir -p "$RESULT_DIR"

docker exec "$CONTAINER" bash -c "
source /opt/vllm-venv/bin/activate && \
python -m vllm.benchmark.benchmark_serving \
  --url http://${SERVER_HOST}:${PORT}/v1 \
  --model \${VLLM_MODEL_ID:-$MODEL_NAME} \
  --dataset sharegpt \
  --request-rate 5 \
  --num-prompts \${VLLM_NUM_PROMPTS:-200} \
  --max-concurrency \${VLLM_MAX_CONCURRENCY:-6} \
  --save-result \
  --result-dir ${RESULT_DIR} \
  --result-filename benchmark_\$(date +%Y%m%d_%H%M%S).json"
EOF

    chmod +x "$SCRIPT_DIR"/run_vllm_server.sh "$SCRIPT_DIR"/bench_vllm_client.sh
    log_success "è„šæœ¬å·²ç”Ÿæˆï¼š$SCRIPT_DIR/run_vllm_server.sh ç­‰"
}

smoke_test() {
    print_sep
    log_info "é™„åŠ ï¼šå®¹å™¨å†…æ‰“å° vLLM ç‰ˆæœ¬ï¼Œç¡®è®¤ç¯å¢ƒ"
    print_sep

    docker exec "$CONTAINER_NAME" bash -c 'source /opt/vllm-venv/bin/activate && python -c "import vllm, torch; print(\"vLLM\", vllm.__version__, \"Torch\", torch.__version__)"'
    log_success "vLLM ç¯å¢ƒå°±ç»ª"
}

################################################################################
# ä¸»å‡½æ•°
################################################################################
main() {
    print_sep
    log_info "vLLM è‡ªåŠ¨éƒ¨ç½²è„šæœ¬å¼€å§‹"
    print_sep

    check_environment
    setup_mihomo_proxy
    install_docker
    install_nvidia_toolkit
    prepare_workspace
    pull_vllm_image
    start_vllm_container
    configure_vllm_env
    generate_helper_scripts
    smoke_test

    print_sep
    log_success "ğŸ‰ vLLM ç¯å¢ƒéƒ¨ç½²å®Œæˆï¼"
    print_sep

    cat <<'EOF'
åç»­å¸¸ç”¨å‘½ä»¤ï¼š
1. å¯åŠ¨æœåŠ¡ï¼š    /workspace/scripts/run_vllm_server.sh
2. å‹æµ‹å‘½ä»¤ï¼š    /workspace/scripts/bench_vllm_client.sh
3. è¿›å…¥å®¹å™¨ï¼š    docker exec -it vllm-gpu bash
4. æŸ¥çœ‹æ—¥å¿—ï¼š    tail -f /workspace/logs/vllm_server_*.log
5. åœæ­¢å®¹å™¨ï¼š    docker stop vllm-gpu

æç¤ºï¼š
- é»˜è®¤æ¨¡å‹è·¯å¾„ /workspace/models/qwen2.5-32b/qwen2.5-32bï¼ŒæŒ‰éœ€æå‰æ”¾å…¥ã€‚
- å¦‚éœ€ä»£ç†ï¼Œè¯·åœ¨è¿è¡Œè„šæœ¬å‰è®¾ç½® http_proxy/https_proxyã€‚
- å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é•œåƒã€ç«¯å£ã€å¹¶å‘æ•°ç­‰å‚æ•°ã€‚
EOF
}

main "$@"

