#!/usr/bin/env python3
"""çœŸæ­£æµ‹è¯• 2k input + ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆä¿®å¤ token é•¿åº¦é—®é¢˜ï¼‰"""

import flexflow.serve as ff
import time

print("="*70)
print("çœŸæ­£çš„ 2k Context æµ‹è¯•ï¼ˆ4å¡å¼ é‡å¹¶è¡Œï¼‰")
print("="*70)
print()

# ç”Ÿæˆçº¦ 1500 tokens çš„ promptï¼ˆç•™ 500 tokens ç»™è¾“å‡ºï¼‰
# åŸºç¡€æ–‡æœ¬çº¦ 45 tokensï¼Œé‡å¤ 30 æ¬¡ = 1350 tokens
base_text = """
Once upon a time, in a vast kingdom filled with mysteries and wonders, 
there lived a young adventurer named Alex who dreamed of exploring the 
unknown territories beyond the mountains. The journey was long and perilous,
filled with ancient forests, towering peaks, and hidden valleys where
legends spoke of treasures untold and dangers unimaginable.
"""

# é‡å¤ 30 æ¬¡ï¼Œçº¦ 1350 tokensï¼ˆå®‰å…¨èŒƒå›´å†…ï¼‰
long_story = base_text * 30

print(f"ğŸ“ Input é•¿åº¦: {len(long_story)} å­—ç¬¦")
print(f"   (çº¦ {len(long_story.split())} ä¸ªå•è¯)")
print(f"   (çº¦ {int(len(long_story.split()) * 1.3)} tokens)")
print()
print("ğŸ’¡ é…ç½®:")
print("   max_seq_length: 2048 tokens")
print("   é¢„ç•™ç©ºé—´: ~700 tokens for output")
print()

# åˆå§‹åŒ– - 4å¡å¼ é‡å¹¶è¡Œ
start_init = time.time()
print("ã€1/6ã€‘åˆå§‹åŒ– FlexFlow (4 GPUs, TP=4)...")
ff.init(
    num_gpus=4,
    memory_per_gpu=14000,
    zero_copy_memory_per_node=80000,
    tensor_parallelism_degree=4,
    pipeline_parallelism_degree=1
)
print(f"âœ… åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {time.time() - start_init:.2f}s)")
print()

# åŠ è½½æ¨¡å‹
start_load = time.time()
print("ã€2/6ã€‘åŠ è½½æ¨¡å‹...")
model_path = "/root/.cache/huggingface/hub/models--meta-llama--llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9"
llm = ff.LLM(model_path)
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {time.time() - start_load:.2f}s)")
print()

# é…ç½®ç”Ÿæˆå‚æ•°
print("ã€3/6ã€‘é…ç½®ç”Ÿæˆå‚æ•°...")
generation_config = ff.GenerationConfig(
    do_sample=False,     # Greedy decodingï¼ˆå¼ é‡å¹¶è¡Œå…¼å®¹ï¼‰
    temperature=0.9,
    topp=0.8,
    topk=1
)
print("âœ… é…ç½®å®Œæˆ (Greedy decoding)")
print()

# ç¼–è¯‘
start_compile = time.time()
print("ã€4/6ã€‘ç¼–è¯‘æ¨¡å‹...")
print("   max_seq_length: 2048 tokens")
print("   max_tokens_per_batch: 4096 tokens")
llm.compile(
    generation_config,
    max_requests_per_batch=1,
    max_seq_length=2048,
    max_tokens_per_batch=4096
)
print(f"âœ… ç¼–è¯‘å®Œæˆ (è€—æ—¶: {time.time() - start_compile:.2f}s)")
print()

# å¯åŠ¨æœåŠ¡
print("ã€5/6ã€‘å¯åŠ¨æ¨ç†æœåŠ¡...")
llm.start_server()
print("âœ… æœåŠ¡å¯åŠ¨")
print()

# æ¨ç†æµ‹è¯•
print("ã€6/6ã€‘æµ‹è¯•æ¨ç†...")
print("="*70)
print(f"ğŸ“ Prompt é•¿åº¦: {len(long_story)} å­—ç¬¦ (~{len(long_story.split())} å•è¯)")
print()

start = time.time()
result = llm.generate(long_story)
elapsed = time.time() - start

if result and len(result) > 0 and hasattr(result[0], 'output_text'):
    output = result[0].output_text
    if isinstance(output, bytes):
        output = output.decode('utf-8')
    
    print(f"\nâœ… æ¨ç†æˆåŠŸ")
    print("="*70)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   Input:  {len(long_story):,} å­—ç¬¦ (~{len(long_story.split())} å•è¯)")
    print(f"   Output: {len(output):,} å­—ç¬¦ (~{len(output.split())} å•è¯)")
    print(f"   Total:  {len(long_story) + len(output):,} å­—ç¬¦")
    print(f"   Time:   {elapsed:.2f}s")
    print()
    
    # è®¡ç®—ååé‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
    input_tokens = int(len(long_story.split()) * 1.3)
    output_tokens = int(len(output.split()) * 1.3)
    total_tokens = input_tokens + output_tokens
    throughput = total_tokens / elapsed
    
    print(f"   ä¼°ç®— tokens:")
    print(f"     Input:  ~{input_tokens} tokens")
    print(f"     Output: ~{output_tokens} tokens")
    print(f"     Total:  ~{total_tokens} tokens")
    print(f"   ååé‡: {throughput:.2f} tokens/s")
    print()
    
    print(f"ğŸ“ Output é¢„è§ˆ (å‰ 500 å­—ç¬¦):")
    print("-"*70)
    print(output[:500])
    if len(output) > 500:
        print("...")
        print(f"(è¿˜æœ‰ {len(output) - 500} å­—ç¬¦)")
    print("-"*70)
else:
    print(f"\nâš ï¸  ç»“æœå¼‚å¸¸: {result}")

llm.stop_server()

print("\n" + "="*70)
print("âœ… 2k Context é•¿æ–‡æœ¬æµ‹è¯•å®Œæˆ")
print("="*70)
print()
print("ğŸ’¡ å…³é”®é…ç½®:")
print("   âœ… do_sample=False (Greedy, å…¼å®¹å¼ é‡å¹¶è¡Œ)")
print("   âœ… tensor_parallelism_degree=4 (4å¡å¹¶è¡Œ)")
print("   âœ… max_seq_length=2048 (æ”¯æŒ 2k tokens)")
print("   âœ… input: ~1300 tokens (ç•™ç©ºé—´ç»™ output)")
print()
print("ğŸ“Š éªŒè¯ç»“æœ:")
print(f"   Input å®é™…é•¿åº¦: ~{int(len(long_story.split()) * 1.3)} tokens")
print("   é…ç½®ä¸Šé™: 2048 tokens")
print(f"   æ˜¯å¦æˆåŠŸ: {'âœ… æ˜¯' if int(len(long_story.split()) * 1.3) < 2048 else 'âŒ å¦'}")
print()
