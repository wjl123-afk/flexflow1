#!/usr/bin/env python3
"""æµ‹è¯• ModelScope ä¸‹è½½çš„ OPT-125M + FlexFlow (ä½¿ç”¨ç»å¯¹è·¯å¾„)"""

import flexflow.serve as ff
import time

print("="*70)
print("ğŸ¤– FlexFlow + OPT-125M (ä½¿ç”¨ç»å¯¹è·¯å¾„)")
print("="*70)
print()

# åˆå§‹åŒ–
start_time = time.time()
print("ã€1/6ã€‘åˆå§‹åŒ– FlexFlow...")
ff.init(
    num_gpus=1,
    memory_per_gpu=8000,
    zero_copy_memory_per_node=20000,
    tensor_parallelism_degree=1,
    pipeline_parallelism_degree=1
)
print(f"âœ… åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}s)")
print()

# åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œè€Œä¸æ˜¯ HF repo IDï¼‰
start_time = time.time()
print("ã€2/6ã€‘åŠ è½½æ¨¡å‹...")
model_path = "/root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6"
print(f"è·¯å¾„: {model_path}")
llm = ff.LLM(model_path)
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}s)")
print()

# é…ç½®ç”Ÿæˆå‚æ•°
print("ã€3/6ã€‘é…ç½®ç”Ÿæˆå‚æ•°...")
generation_config = ff.GenerationConfig(
    do_sample=False, 
    temperature=0.9, 
    topp=0.8, 
    topk=1
)
print("âœ… é…ç½®å®Œæˆ")
print()

# ç¼–è¯‘
start_time = time.time()
print("ã€4/6ã€‘ç¼–è¯‘æ¨¡å‹...")
print("âš ï¸  å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œä¼šè‡ªåŠ¨è½¬æ¢æƒé‡ï¼ˆéœ€è¦ 2-3 åˆ†é’Ÿï¼‰")
llm.compile(
    generation_config,
    max_requests_per_batch=1,
    max_seq_length=128,
    max_tokens_per_batch=64
)
print(f"âœ… ç¼–è¯‘å®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}s)")
print()

# å¯åŠ¨æœåŠ¡
print("ã€5/6ã€‘å¯åŠ¨æ¨ç†æœåŠ¡...")
llm.start_server()
print("âœ… æœåŠ¡å¯åŠ¨")
print()

# æµ‹è¯•æ¨ç†
print("ã€6/6ã€‘æµ‹è¯•æ¨ç†...")
print("="*70)
prompt = "Hello, I am a language model"
print(f"ğŸ“ Prompt: {prompt}")
print()

start_time = time.time()
result = llm.generate(prompt)
inference_time = time.time() - start_time

print(f"ğŸ¤– Result: {result}")
if result and len(result) > 0:
    if hasattr(result[0], 'output_text'):
        output = result[0].output_text
        if isinstance(output, bytes):
            output = output.decode('utf-8')
        print(f"âœ… Output: {output}")
        print(f"â±ï¸  æ¨ç†æ—¶é—´: {inference_time:.2f}s")
    else:
        print(f"âš ï¸  Result attributes: {dir(result[0])}")

print("="*70)
print()

# åœæ­¢æœåŠ¡
llm.stop_server()
print("âœ… æµ‹è¯•å®Œæˆ")
print()
print("="*70)
print("ğŸ‰ FlexFlow + OPT-125M è¿è¡ŒæˆåŠŸï¼")
print("="*70)
print()
print("ğŸ“ éªŒè¯æƒé‡è½¬æ¢:")
print(f"   ls -lh {model_path}/half-precision/")
print()
