#!/usr/bin/env python3
"""
NCCLæ—¥å¿—åˆ†æå·¥å…·
ä»NCCL profilingæ—¥å¿—ä¸­æå–çœŸå®çš„é€šä¿¡æ•°æ®
"""

import os
import sys
import re
import json
from pathlib import Path
from collections import defaultdict


def parse_nccl_log(log_file):
    """è§£æå•ä¸ªNCCLæ—¥å¿—æ–‡ä»¶"""
    stats = {
        'allreduce_count': 0,
        'broadcast_count': 0,
        'send_count': 0,
        'recv_count': 0,
        'total_comm_ops': 0,
        'bandwidth_samples': [],
        'comm_size_bytes': []
    }
    
    if not os.path.exists(log_file):
        return stats
    
    with open(log_file, 'r') as f:
        for line in f:
            # ç»Ÿè®¡é€šä¿¡æ“ä½œç±»å‹
            if 'AllReduce' in line:
                stats['allreduce_count'] += 1
            elif 'Broadcast' in line:
                stats['broadcast_count'] += 1
            elif 'Send' in line:
                stats['send_count'] += 1
            elif 'Recv' in line:
                stats['recv_count'] += 1
            
            # æå–é€šä¿¡å¸¦å®½ (æ ¼å¼: busbw 123.45 GB/s)
            bw_match = re.search(r'busbw\s+([\d.]+)', line)
            if bw_match:
                stats['bandwidth_samples'].append(float(bw_match.group(1)))
            
            # æå–é€šä¿¡æ•°æ®é‡ (æ ¼å¼: size 12345 bytes)
            size_match = re.search(r'size\s+(\d+)', line)
            if size_match:
                stats['comm_size_bytes'].append(int(size_match.group(1)))
    
    stats['total_comm_ops'] = (stats['allreduce_count'] + 
                                stats['broadcast_count'] + 
                                stats['send_count'] + 
                                stats['recv_count'])
    
    # è®¡ç®—å¹³å‡å¸¦å®½
    if stats['bandwidth_samples']:
        stats['avg_bandwidth_gbps'] = sum(stats['bandwidth_samples']) / len(stats['bandwidth_samples'])
    else:
        stats['avg_bandwidth_gbps'] = 0.0
    
    # è®¡ç®—æ€»é€šä¿¡é‡
    if stats['comm_size_bytes']:
        stats['total_comm_gb'] = sum(stats['comm_size_bytes']) / (1024**3)
        stats['avg_comm_size_mb'] = (sum(stats['comm_size_bytes']) / len(stats['comm_size_bytes'])) / (1024**2)
    else:
        stats['total_comm_gb'] = 0.0
        stats['avg_comm_size_mb'] = 0.0
    
    return stats


def calculate_comm_overhead(result_file, nccl_stats):
    """
    æ ¹æ®æ€§èƒ½æ•°æ®å’ŒNCCLç»Ÿè®¡è®¡ç®—çœŸå®é€šä¿¡å¼€é”€
    
    å‡è®¾ï¼š
    - å•å¡å»¶è¿Ÿ = çº¯è®¡ç®—æ—¶é—´ (æ— é€šä¿¡)
    - å¤šå¡å»¶è¿Ÿ = è®¡ç®—æ—¶é—´ + é€šä¿¡æ—¶é—´
    - é€šä¿¡å¼€é”€% = (å¤šå¡å»¶è¿Ÿ - å•å¡å»¶è¿Ÿ) / å¤šå¡å»¶è¿Ÿ * 100
    """
    if not os.path.exists(result_file):
        return None
    
    with open(result_file, 'r') as f:
        result = json.load(f)
    
    e2e_latency = result['metrics']['e2e_latency_ms']
    
    # å‡è®¾å•å¡å»¶è¿Ÿä¸ºåŸºçº¿ï¼ˆçº¯è®¡ç®—ï¼‰
    # è¿™ä¸ªå€¼åº”è¯¥ä» result_TP1_PP1_SL128.json è¯»å–
    baseline_latency = 1702.97  # å•å¡åŸºçº¿å»¶è¿Ÿ
    
    if e2e_latency > baseline_latency:
        overhead_ms = e2e_latency - baseline_latency
        overhead_percent = (overhead_ms / e2e_latency) * 100
    else:
        overhead_ms = 0
        overhead_percent = 0
    
    return {
        'e2e_latency_ms': e2e_latency,
        'baseline_latency_ms': baseline_latency,
        'overhead_ms': overhead_ms,
        'overhead_percent': overhead_percent,
        'total_comm_ops': nccl_stats['total_comm_ops'],
        'allreduce_count': nccl_stats['allreduce_count'],
        'avg_bandwidth_gbps': nccl_stats['avg_bandwidth_gbps'],
        'total_comm_gb': nccl_stats['total_comm_gb']
    }


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python analyze_nccl_logs.py <nccl_log_dir>")
        sys.exit(1)
    
    nccl_log_dir = Path(sys.argv[1])
    result_dir = nccl_log_dir.parent
    
    print("\n" + "="*70)
    print("ğŸ“Š NCCLé€šä¿¡æ•°æ®åˆ†ææŠ¥å‘Š")
    print("="*70 + "\n")
    
    # ç­–ç•¥åç§°æ˜ å°„
    strategy_names = {
        1: "TP1_PP1 (å•å¡åŸºçº¿)",
        2: "TP2_PP1 (2å¡å¼ é‡å¹¶è¡Œ)",
        3: "TP4_PP1 (4å¡å¼ é‡å¹¶è¡Œ)",
        4: "TP1_PP2 (2å¡æµæ°´çº¿å¹¶è¡Œ)",
        5: "TP1_PP4 (4å¡æµæ°´çº¿å¹¶è¡Œ)",
        6: "TP2_PP2 (2Ã—2æ··åˆå¹¶è¡Œ)"
    }
    
    # è¯»å–å•å¡åŸºçº¿
    baseline_file = result_dir / "result_TP1_PP1_SL128.json"
    baseline_latency = 1702.97  # é»˜è®¤å€¼
    if baseline_file.exists():
        with open(baseline_file, 'r') as f:
            baseline_data = json.load(f)
            baseline_latency = baseline_data['metrics']['e2e_latency_ms']
        print(f"âœ… å•å¡åŸºçº¿å»¶è¿Ÿ: {baseline_latency:.2f} ms\n")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°å•å¡åŸºçº¿æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼: {baseline_latency:.2f} ms\n")
    
    # åˆ†ææ‰€æœ‰ç­–ç•¥
    all_results = []
    
    for idx in range(1, 7):
        nccl_log = nccl_log_dir / f"strategy_{idx}_nccl.log"
        result_file = result_dir / f"result_TP{2 if idx in [2,6] else (4 if idx==3 else 1)}_PP{2 if idx in [4,6] else (4 if idx==5 else 1)}_SL128.json"
        
        strategy_name = strategy_names.get(idx, f"ç­–ç•¥{idx}")
        
        print(f"{'â”€'*70}")
        print(f"ç­–ç•¥ {idx}: {strategy_name}")
        print(f"{'â”€'*70}")
        
        # è§£æNCCLæ—¥å¿—
        nccl_stats = parse_nccl_log(nccl_log)
        
        if nccl_stats['total_comm_ops'] > 0:
            print(f"ğŸ“¡ NCCLé€šä¿¡ç»Ÿè®¡:")
            print(f"   - AllReduce æ“ä½œ: {nccl_stats['allreduce_count']} æ¬¡")
            print(f"   - Broadcast æ“ä½œ: {nccl_stats['broadcast_count']} æ¬¡")
            print(f"   - Send/Recv æ“ä½œ: {nccl_stats['send_count'] + nccl_stats['recv_count']} æ¬¡")
            print(f"   - æ€»é€šä¿¡æ“ä½œ: {nccl_stats['total_comm_ops']} æ¬¡")
            
            if nccl_stats['avg_bandwidth_gbps'] > 0:
                print(f"   - å¹³å‡å¸¦å®½: {nccl_stats['avg_bandwidth_gbps']:.2f} GB/s")
            
            if nccl_stats['total_comm_gb'] > 0:
                print(f"   - æ€»é€šä¿¡é‡: {nccl_stats['total_comm_gb']:.4f} GB")
                print(f"   - å¹³å‡é€šä¿¡å¤§å°: {nccl_stats['avg_comm_size_mb']:.2f} MB")
        else:
            print(f"   â„¹ï¸  æ— NCCLé€šä¿¡ï¼ˆå•å¡æˆ–æœªæ£€æµ‹åˆ°é€šä¿¡æ—¥å¿—ï¼‰")
        
        # è®¡ç®—é€šä¿¡å¼€é”€
        if result_file.exists():
            with open(result_file, 'r') as f:
                result = json.load(f)
            
            e2e_latency = result['metrics']['e2e_latency_ms']
            
            # è®¡ç®—çœŸå®é€šä¿¡å¼€é”€
            if e2e_latency > baseline_latency:
                overhead_ms = e2e_latency - baseline_latency
                overhead_percent = (overhead_ms / e2e_latency) * 100
            else:
                overhead_ms = 0
                overhead_percent = 0
            
            print(f"\nâ±ï¸  æ€§èƒ½æŒ‡æ ‡:")
            print(f"   - E2Eå»¶è¿Ÿ: {e2e_latency:.2f} ms")
            print(f"   - åŸºçº¿å»¶è¿Ÿ: {baseline_latency:.2f} ms")
            print(f"   - å¼€é”€æ—¶é—´: {overhead_ms:.2f} ms")
            print(f"   - å¼€é”€ç™¾åˆ†æ¯”: {overhead_percent:.1f}% (çœŸå®æµ‹é‡)")
            
            # ä¿å­˜ç»“æœ
            all_results.append({
                'strategy_id': idx,
                'strategy_name': strategy_name,
                'e2e_latency_ms': e2e_latency,
                'overhead_ms': overhead_ms,
                'overhead_percent': overhead_percent,
                'nccl_stats': nccl_stats
            })
        else:
            print(f"\nâš ï¸  æœªæ‰¾åˆ°æ€§èƒ½æ•°æ®æ–‡ä»¶: {result_file.name}")
        
        print()
    
    # ç”Ÿæˆæ±‡æ€»å¯¹æ¯”
    print("="*70)
    print("ğŸ“ˆ é€šä¿¡å¼€é”€å¯¹æ¯”æ±‡æ€»")
    print("="*70 + "\n")
    
    print(f"{'ç­–ç•¥':<25} {'å»¶è¿Ÿ(ms)':<12} {'å¼€é”€(ms)':<12} {'å¼€é”€%':<10} {'é€šä¿¡æ“ä½œ':<10}")
    print("â”€"*70)
    
    for result in all_results:
        print(f"{result['strategy_name']:<25} "
              f"{result['e2e_latency_ms']:<12.2f} "
              f"{result['overhead_ms']:<12.2f} "
              f"{result['overhead_percent']:<10.1f} "
              f"{result['nccl_stats']['total_comm_ops']:<10}")
    
    print()
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_file = result_dir / "nccl_analysis_report.json"
    with open(report_file, 'w') as f:
        json.dump({
            'baseline_latency_ms': baseline_latency,
            'strategies': all_results
        }, f, indent=2)
    
    print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print()
    
    # ç”ŸæˆCSV
    csv_file = result_dir / "nccl_comm_overhead.csv"
    with open(csv_file, 'w') as f:
        f.write("Strategy,TP,PP,GPUs,E2E_Latency_ms,Overhead_ms,Overhead_percent,AllReduce_ops,Total_comm_ops\n")
        
        tp_pp_map = {
            1: (1, 1, 1),
            2: (2, 1, 2),
            3: (4, 1, 4),
            4: (1, 2, 2),
            5: (1, 4, 4),
            6: (2, 2, 4)
        }
        
        for result in all_results:
            idx = result['strategy_id']
            tp, pp, gpus = tp_pp_map.get(idx, (1, 1, 1))
            strategy_short = f"TP{tp}_PP{pp}"
            
            f.write(f"{strategy_short},{tp},{pp},{gpus},"
                   f"{result['e2e_latency_ms']:.2f},"
                   f"{result['overhead_ms']:.2f},"
                   f"{result['overhead_percent']:.1f},"
                   f"{result['nccl_stats']['allreduce_count']},"
                   f"{result['nccl_stats']['total_comm_ops']}\n")
    
    print(f"ğŸ“Š CSVæŠ¥å‘Šå·²ä¿å­˜: {csv_file}")
    print()
    
    print("="*70)
    print("âœ… NCCLåˆ†æå®Œæˆï¼")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()

