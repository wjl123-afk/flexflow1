#!/usr/bin/env python3
"""
FlexFlow Serve ç¦»çº¿ Patch - ç»ˆæç‰ˆï¼ˆç›´æ¥ patchï¼‰
ç›´æ¥å¯¹ serve.py è¿›è¡Œå­—ç¬¦ä¸²æ›¿æ¢ï¼Œæ— éœ€å¤‡ä»½æ–‡ä»¶

4 ä¸ªå…³é”® Patchï¼š
1. LLM.__init__: AutoConfig ä½¿ç”¨ local_files_only=True
2. download_hf_config: ç›´æ¥ returnï¼Œè·³è¿‡åœ¨çº¿ä¸‹è½½
3. download_hf_tokenizer_if_needed: ç›´æ¥ returnï¼Œè·³è¿‡åœ¨çº¿åˆ·æ–°
4. download_and_convert_llm_weights: AutoModelForCausalLM ä½¿ç”¨ local_files_only=True

ä½¿ç”¨æ–¹æ³•ï¼š
    python patch_serve_ultimate.py
"""

import shutil
import os
import sys

SERVE_PY = "/workspace/flexflow-serve/python/flexflow/serve/serve.py"
BACKUP = SERVE_PY + ".backup"

print("="*70)
print("ğŸ”§ FlexFlow Serve ç¦»çº¿ Patch - ç»ˆæç‰ˆ")
print("="*70)
print()

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(SERVE_PY):
    print(f"âŒ é”™è¯¯ï¼šserve.py ä¸å­˜åœ¨")
    print(f"   è·¯å¾„: {SERVE_PY}")
    sys.exit(1)

# 1. å¤‡ä»½
print("ğŸ“¦ Step 1: å¤‡ä»½åŸå§‹æ–‡ä»¶...")
if not os.path.exists(BACKUP):
    try:
        shutil.copy2(SERVE_PY, BACKUP)
        print(f"   âœ… å·²å¤‡ä»½åˆ°: {BACKUP}")
    except Exception as e:
        print(f"   âš ï¸  å¤‡ä»½å¤±è´¥: {e}")
else:
    print(f"   â„¹ï¸  å¤‡ä»½å·²å­˜åœ¨ï¼Œè·³è¿‡")
print()

# 2. è¯»å–æ–‡ä»¶
print("ğŸ“– Step 2: è¯»å– serve.py...")
try:
    with open(SERVE_PY, "r", encoding="utf-8") as f:
        content = f.read()
    print("   âœ… è¯»å–å®Œæˆ")
except Exception as e:
    print(f"   âŒ è¯»å–å¤±è´¥: {e}")
    sys.exit(1)
print()

patches_applied = 0
original_content = content

# ======================================================================
# Patch 1: LLM.__init__ - AutoConfig å¼ºåˆ¶æœ¬åœ°åŠ è½½
# ======================================================================
print("ğŸ”¨ Patch 1: LLM.__init__ (AutoConfig local_files_only)...")
old1 = 'self.hf_config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)'
new1 = 'self.hf_config = AutoConfig.from_pretrained(model_name, trust_remote_code=True, local_files_only=True)'

if old1 in content and new1 not in content:
    content = content.replace(old1, new1, 1)
    patches_applied += 1
    print("   âœ… æˆåŠŸ")
elif new1 in content:
    print("   âš ï¸  å·²åº”ç”¨")
else:
    print("   âŒ æœªæ‰¾åˆ°ç›®æ ‡ä»£ç ")
print()

# ======================================================================
# Patch 2: download_hf_config - ç›´æ¥ return
# ======================================================================
print("ğŸ”¨ Patch 2: download_hf_config (è·³è¿‡åœ¨çº¿ä¸‹è½½)...")

# å°è¯•å¤šç§å¯èƒ½çš„æ ¼å¼
old2_variants = [
    # å˜ä½“ 1: å•å¼•å· docstring
    '''    def download_hf_config(self):
        """Save the HuggingFace model configs to a json file. Useful mainly to run the C++ inference code."""
        config_dir''',
    # å˜ä½“ 2: ä¸‰å¼•å·åç›´æ¥ä»£ç 
    '''    def download_hf_config(self):
        """Check in the folder specified by the cache_path whether the LLM's
        config is available and up to date. If not, or if the refresh_cache
        parameter is set to True, download new config from huggingface.
        """
        print("Loading model configs from huggingface.co")''',
    # å˜ä½“ 3: æœ€ç®€å•çš„åŒ¹é…
    '''    def download_hf_config(self):
        """'''
]

patch2_applied = False
for i, old2 in enumerate(old2_variants):
    if old2 in content:
        # æ£€æŸ¥æ˜¯å¦å·²ç» patch
        if 'def download_hf_config(self):' in content:
            func_start = content.find('def download_hf_config(self):')
            func_part = content[func_start:func_start+200]
            if 'return  # PATCHED' in func_part:
                print("   âš ï¸  å·²åº”ç”¨")
                patch2_applied = True
                break
        
        # åº”ç”¨ patch
        new2 = old2.replace(
            '    def download_hf_config(self):\n        """',
            '    def download_hf_config(self):\n        return  # PATCHED: è·³è¿‡åœ¨çº¿ä¸‹è½½ï¼Œä½¿ç”¨æœ¬åœ°é…ç½®\n        """',
            1
        )
        content = content.replace(old2, new2, 1)
        patches_applied += 1
        print(f"   âœ… æˆåŠŸ (ä½¿ç”¨å˜ä½“ {i+1})")
        patch2_applied = True
        break

if not patch2_applied:
    print("   âš ï¸  æœªæ‰¾åˆ°ç›®æ ‡ä»£ç ï¼Œå°è¯•é€šç”¨æ–¹æ³•...")
    # é€šç”¨æ–¹æ³•ï¼šåœ¨å‡½æ•°å®šä¹‰åæ’å…¥ return
    pattern = 'def download_hf_config(self):'
    if pattern in content:
        lines = content.split('\n')
        for i, line in enumerate(lines):
            if 'def download_hf_config(self):' in line:
                # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦å·²ç»æ˜¯ return
                if i+1 < len(lines) and 'return' in lines[i+1]:
                    print("   âš ï¸  å·²åº”ç”¨")
                else:
                    # æ’å…¥ return è¯­å¥
                    indent = '        '  # 8 spaces
                    lines.insert(i+1, f'{indent}return  # PATCHED: è·³è¿‡åœ¨çº¿ä¸‹è½½')
                    content = '\n'.join(lines)
                    patches_applied += 1
                    print("   âœ… æˆåŠŸ (é€šç”¨æ–¹æ³•)")
                patch2_applied = True
                break
    
    if not patch2_applied:
        print("   âŒ å¤±è´¥")
print()

# ======================================================================
# Patch 3: download_hf_tokenizer_if_needed - ç›´æ¥ return
# ======================================================================
print("ğŸ”¨ Patch 3: download_hf_tokenizer_if_needed (è·³è¿‡åˆ·æ–°)...")

old3_variants = [
    '''    def download_hf_tokenizer_if_needed(self):
        """Download tokenizer from HuggingFace (or from the model's source in general) if needed."""
        print("Loading tokenizer...")''',
    '''    def download_hf_tokenizer_if_needed(self):
        """Check in the folder specified by the cache_path whether the LLM's tokenizer files are available and up to date.
        If not, or if the refresh_cache parameter is set to True, download new tokenizer files.
        """
        print("Loading tokenizer...")''',
    '''    def download_hf_tokenizer_if_needed(self):
        """'''
]

patch3_applied = False
for i, old3 in enumerate(old3_variants):
    if old3 in content:
        # æ£€æŸ¥æ˜¯å¦å·²ç» patch
        if 'def download_hf_tokenizer_if_needed(self):' in content:
            func_start = content.find('def download_hf_tokenizer_if_needed(self):')
            func_part = content[func_start:func_start+300]
            if 'return  # PATCHED' in func_part:
                print("   âš ï¸  å·²åº”ç”¨")
                patch3_applied = True
                break
        
        # åº”ç”¨ patch
        new3 = old3.replace(
            '    def download_hf_tokenizer_if_needed(self):\n        """',
            '    def download_hf_tokenizer_if_needed(self):\n        return  # PATCHED: è·³è¿‡åœ¨çº¿åˆ·æ–°ï¼Œä½¿ç”¨æœ¬åœ° tokenizer\n        """',
            1
        )
        content = content.replace(old3, new3, 1)
        patches_applied += 1
        print(f"   âœ… æˆåŠŸ (ä½¿ç”¨å˜ä½“ {i+1})")
        patch3_applied = True
        break

if not patch3_applied:
    print("   âš ï¸  æœªæ‰¾åˆ°ç›®æ ‡ä»£ç ï¼Œå°è¯•é€šç”¨æ–¹æ³•...")
    pattern = 'def download_hf_tokenizer_if_needed(self):'
    if pattern in content:
        lines = content.split('\n')
        for i, line in enumerate(lines):
            if 'def download_hf_tokenizer_if_needed(self):' in line:
                if i+1 < len(lines) and 'return' in lines[i+1]:
                    print("   âš ï¸  å·²åº”ç”¨")
                else:
                    indent = '        '
                    lines.insert(i+1, f'{indent}return  # PATCHED: è·³è¿‡åœ¨çº¿åˆ·æ–°')
                    content = '\n'.join(lines)
                    patches_applied += 1
                    print("   âœ… æˆåŠŸ (é€šç”¨æ–¹æ³•)")
                patch3_applied = True
                break
    
    if not patch3_applied:
        print("   âŒ å¤±è´¥")
print()

# ======================================================================
# Patch 4: download_and_convert_llm_weights - æœ¬åœ°æƒé‡è½¬æ¢
# ======================================================================
print("ğŸ”¨ Patch 4: download_and_convert_llm_weights (æœ¬åœ°æƒé‡)...")

old4_variants = [
    '''            snapshot_download(
                repo_id=model_name,
                allow_patterns="*.safetensors",
                max_workers=min(30, num_cores),
            )
            hf_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,''',
    # æ›´ç®€å•çš„åŒ¹é…
    '''hf_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                torch_dtype='''
]

patch4_applied = False

# é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç» patch
if 'local_files_only=True,  # PATCHED' in content or 'local_files_only=True # PATCHED' in content:
    print("   âš ï¸  å·²åº”ç”¨")
    patch4_applied = True
else:
    # å°è¯•ç¬¬ä¸€ä¸ªå˜ä½“ï¼ˆåŒ…å« snapshot_downloadï¼‰
    if old4_variants[0] in content:
        new4 = '''            # PATCHED: è·³è¿‡ snapshot_downloadï¼Œç›´æ¥ä»æœ¬åœ°åŠ è½½
            print(f"[PATCHED] ä»æœ¬åœ°åŠ è½½æƒé‡: {model_name}")
            # snapshot_download(
            #     repo_id=model_name,
            #     allow_patterns="*.safetensors",
            #     max_workers=min(30, num_cores),
            # )
            hf_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                local_files_only=True,  # PATCHED'''
        content = content.replace(old4_variants[0], new4, 1)
        patches_applied += 1
        print("   âœ… æˆåŠŸ (å®Œæ•´ç‰ˆ)")
        patch4_applied = True
    # å°è¯•ç®€åŒ–ç‰ˆæœ¬
    elif 'AutoModelForCausalLM.from_pretrained' in content:
        # æŸ¥æ‰¾å¹¶æ·»åŠ  local_files_only
        lines = content.split('\n')
        for i, line in enumerate(lines):
            if 'hf_model = AutoModelForCausalLM.from_pretrained' in line:
                # æŸ¥æ‰¾è¿™ä¸ªå‡½æ•°è°ƒç”¨çš„ç»“æŸä½ç½®
                for j in range(i, min(i+10, len(lines))):
                    if 'trust_remote_code=True,' in lines[j] and 'local_files_only' not in lines[j]:
                        lines[j] = lines[j].replace(
                            'trust_remote_code=True,',
                            'trust_remote_code=True,\n                local_files_only=True,  # PATCHED'
                        )
                        content = '\n'.join(lines)
                        patches_applied += 1
                        print("   âœ… æˆåŠŸ (æ·»åŠ  local_files_only)")
                        patch4_applied = True
                        break
                if patch4_applied:
                    break

if not patch4_applied:
    print("   âŒ æœªæ‰¾åˆ°ç›®æ ‡ä»£ç ")
print()

# 3. ä¿å­˜
if patches_applied > 0 or content != original_content:
    print(f"ğŸ’¾ Step 3: ä¿å­˜ä¿®æ”¹ ({patches_applied} ä¸ªæ–° patch)...")
    try:
        with open(SERVE_PY, "w", encoding="utf-8") as f:
            f.write(content)
        print("   âœ… ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
        sys.exit(1)
else:
    print("â„¹ï¸  æ²¡æœ‰æ–°çš„ patch éœ€è¦åº”ç”¨")

print()
print("="*70)
print("ğŸ‰ Patch å®Œæˆï¼")
print("="*70)
print()
print("ğŸ“ å…³é”®ä¿®æ”¹:")
print("   1. LLM.__init__: AutoConfig ä½¿ç”¨ local_files_only=True")
print("   2. download_hf_config: ç›´æ¥ returnï¼Œè·³è¿‡åœ¨çº¿ä¸‹è½½")
print("   3. download_hf_tokenizer_if_needed: ç›´æ¥ returnï¼Œè·³è¿‡åˆ·æ–°")
print("   4. download_and_convert_llm_weights: ä½¿ç”¨æœ¬åœ° safetensors")
print()
print("ğŸ” éªŒè¯ patch:")
print('   grep -n "return  # PATCHED" /workspace/flexflow-serve/python/flexflow/serve/serve.py')
print()
print("ğŸ”„ å¦‚éœ€æ¢å¤åŸæ–‡ä»¶:")
print(f"   cp {BACKUP} {SERVE_PY}")
print()
print("ğŸš€ è¿è¡Œæµ‹è¯•:")
print("   cd /workspace/flexflow-serve/examples")
print("   python -u /workspace/test_llama2_1gpu_final.py -ll:gpu 1 -ll:zsize 40000 -ll:fsize 14000 -ll:cpu 4")
print()
