#!/usr/bin/env python3
"""çœŸæ­£æµ‹è¯• 2k input + ç”Ÿæˆé•¿æ–‡æœ¬"""

import flexflow.serve as ff
import time

print("="*70)
print("çœŸæ­£çš„ 2k Context æµ‹è¯•")
print("="*70)
print()

# ç”Ÿæˆä¸€ä¸ªæ¥è¿‘ 2k tokens çš„é•¿ promptï¼ˆçº¦ 1500-1800 ä¸ªå•è¯ï¼‰
# 1 token â‰ˆ 0.75 ä¸ªè‹±æ–‡å•è¯
long_story = """
Once upon a time, in a vast kingdom filled with mysteries and wonders, 
there lived a young adventurer named Alex who dreamed of exploring the 
unknown territories beyond the mountains. """ * 100  # é‡å¤ 100 æ¬¡ï¼Œç”Ÿæˆé•¿æ–‡æœ¬

print(f"ğŸ“ Input é•¿åº¦: {len(long_story)} å­—ç¬¦")
print(f"   (çº¦ {len(long_story.split())} ä¸ªå•è¯)")
print(f"   (çº¦ {len(long_story.split()) * 4 // 3} tokens)")
print()

# åˆå§‹åŒ–
ff.init(
    num_gpus=4,
    memory_per_gpu=14000,
    zero_copy_memory_per_node=80000,
    tensor_parallelism_degree=4,
    pipeline_parallelism_degree=1
)

model_path = "/root/.cache/huggingface/hub/models--meta-llama--llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9"
llm = ff.LLM(model_path)

# æ³¨æ„ï¼štemperature å’Œ sampling è®¾ç½®ä¼šå½±å“ç”Ÿæˆé•¿åº¦
generation_config = ff.GenerationConfig(
    do_sample=True,      # å¼€å¯é‡‡æ ·ï¼Œå¯èƒ½ç”Ÿæˆæ›´é•¿
    temperature=0.9,
    topp=0.8,
    topk=40
)

llm.compile(
    generation_config,
    max_requests_per_batch=1,
    max_seq_length=2048,      # æ”¯æŒ 2k input
    max_tokens_per_batch=4096  # æ”¯æŒç”Ÿæˆæ›´å¤š
)

llm.start_server()

print("ğŸš€ å¼€å§‹æ¨ç†...")
start = time.time()
result = llm.generate(long_story)
elapsed = time.time() - start

if result and len(result) > 0 and hasattr(result[0], 'output_text'):
    output = result[0].output_text
    if isinstance(output, bytes):
        output = output.decode('utf-8')
    
    print(f"\nâœ… æ¨ç†æˆåŠŸ")
    print(f"   Input: {len(long_story)} å­—ç¬¦")
    print(f"   Output: {len(output)} å­—ç¬¦")
    print(f"   Total: {len(long_story) + len(output)} å­—ç¬¦")
    print(f"   Time: {elapsed:.2f}s")
    print()
    print(f"   Output é¢„è§ˆ (å‰ 200 å­—ç¬¦):")
    print(f"   {output[:200]}")
    print(f"   ...")
    print(f"   (è¿˜æœ‰ {len(output) - 200} å­—ç¬¦)")

llm.stop_server()

print("\n" + "="*70)
print("âœ… 2k Context æµ‹è¯•å®Œæˆ")
print("="*70)
