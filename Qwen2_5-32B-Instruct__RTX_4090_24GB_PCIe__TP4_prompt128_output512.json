{
  "paths": {
    "result_json": "/workspace/gpu_model_vllm_test/RTX_4090_24GB_PCIe/Qwen2_5-32B-Instruct/results/Qwen2_5-32B-Instruct__RTX_4090_24GB_PCIe__TP4_prompt128_output512__result.json",
    "server_log": "/workspace/gpu_model_vllm_test/RTX_4090_24GB_PCIe/Qwen2_5-32B-Instruct/logs/Qwen2_5-32B-Instruct__RTX_4090_24GB_PCIe__TP4_prompt128_output512__server.log",
    "client_log": "/workspace/gpu_model_vllm_test/RTX_4090_24GB_PCIe/Qwen2_5-32B-Instruct/logs/Qwen2_5-32B-Instruct__RTX_4090_24GB_PCIe__TP4_prompt128_output512__client.log"
  },
  "server_command": [
    "python",
    "-m",
    "vllm.entrypoints.openai.api_server",
    "--model",
    "/workspace/models/qwen2.5-32b/qwen2.5-32b",
    "--dtype",
    "bfloat16",
    "--tensor-parallel-size",
    "4",
    "--pipeline-parallel-size",
    "1",
    "--kv-cache-dtype",
    "fp8",
    "--max-num-seqs",
    "15",
    "--max-model-len",
    "4096",
    "--max-num-batched-tokens",
    "231",
    "--gpu-memory-utilization",
    "0.85",
    "--enable-prefix-caching",
    "--block-size",
    "16",
    "--enable-chunked-prefill",
    "--swap-space",
    "0",
    "--host",
    "0.0.0.0",
    "--port",
    "8003"
  ],
  "client_command": [
    "vllm",
    "bench",
    "serve",
    "--backend",
    "vllm",
    "--base-url",
    "http://127.0.0.1:8003",
    "--model",
    "/workspace/models/qwen2.5-32b/qwen2.5-32b",
    "--dataset-name",
    "random",
    "--random-input-len",
    "128",
    "--random-output-len",
    "512",
    "--num-prompts",
    "1000",
    "--max-concurrency",
    "12",
    "--metric-percentile",
    "50,90,99",
    "--temperature",
    "0.0",
    "--top-p",
    "1.0",
    "--top-k",
    "0",
    "--save-result",
    "--result-dir",
    "/workspace/gpu_model_vllm_test/RTX_4090_24GB_PCIe/Qwen2_5-32B-Instruct/results"
  ]
}