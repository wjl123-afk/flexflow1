#!/usr/bin/env python3
"""çœŸæ­£æµ‹è¯• 2k input + ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆä¿®å¤é‡‡æ ·é—®é¢˜ï¼‰"""

import flexflow.serve as ff
import time

print("="*70)
print("çœŸæ­£çš„ 2k Context æµ‹è¯•ï¼ˆ4å¡å¼ é‡å¹¶è¡Œï¼‰")
print("="*70)
print()

# ç”Ÿæˆä¸€ä¸ªæ¥è¿‘ 2k tokens çš„é•¿ prompt
# 1 token â‰ˆ 0.75 ä¸ªè‹±æ–‡å•è¯
long_story = """
Once upon a time, in a vast kingdom filled with mysteries and wonders, 
there lived a young adventurer named Alex who dreamed of exploring the 
unknown territories beyond the mountains. """ * 100

print(f"ğŸ“ Input é•¿åº¦: {len(long_story)} å­—ç¬¦")
print(f"   (çº¦ {len(long_story.split())} ä¸ªå•è¯)")
print(f"   (çº¦ {len(long_story.split()) * 4 // 3} tokens)")
print()

# åˆå§‹åŒ– - 4å¡å¼ é‡å¹¶è¡Œ
start_init = time.time()
print("ã€1/6ã€‘åˆå§‹åŒ– FlexFlow (4 GPUs, TP=4)...")
ff.init(
    num_gpus=4,
    memory_per_gpu=14000,
    zero_copy_memory_per_node=80000,
    tensor_parallelism_degree=4,
    pipeline_parallelism_degree=1
)
print(f"âœ… åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {time.time() - start_init:.2f}s)")
print()

# åŠ è½½æ¨¡å‹
start_load = time.time()
print("ã€2/6ã€‘åŠ è½½æ¨¡å‹...")
model_path = "/root/.cache/huggingface/hub/models--meta-llama--llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9"
llm = ff.LLM(model_path)
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {time.time() - start_load:.2f}s)")
print()

# é…ç½®ç”Ÿæˆå‚æ•° - å…³é”®ï¼šä½¿ç”¨ do_sample=False
print("ã€3/6ã€‘é…ç½®ç”Ÿæˆå‚æ•°...")
generation_config = ff.GenerationConfig(
    do_sample=False,     # â† å…³é”®ä¿®æ”¹ï¼šGreedy decodingï¼ˆå¼ é‡å¹¶è¡Œå…¼å®¹ï¼‰
    temperature=0.9,
    topp=0.8,
    topk=1
)
print("âœ… é…ç½®å®Œæˆ (Greedy decoding)")
print()

# ç¼–è¯‘
start_compile = time.time()
print("ã€4/6ã€‘ç¼–è¯‘æ¨¡å‹...")
print("   max_seq_length: 2048")
print("   max_tokens_per_batch: 4096")
llm.compile(
    generation_config,
    max_requests_per_batch=1,
    max_seq_length=2048,
    max_tokens_per_batch=4096
)
print(f"âœ… ç¼–è¯‘å®Œæˆ (è€—æ—¶: {time.time() - start_compile:.2f}s)")
print()

# å¯åŠ¨æœåŠ¡
print("ã€5/6ã€‘å¯åŠ¨æ¨ç†æœåŠ¡...")
llm.start_server()
print("âœ… æœåŠ¡å¯åŠ¨")
print()

# æ¨ç†æµ‹è¯•
print("ã€6/6ã€‘æµ‹è¯•æ¨ç†...")
print("="*70)
print(f"ğŸ“ Prompt é•¿åº¦: {len(long_story)} å­—ç¬¦")
print()

start = time.time()
result = llm.generate(long_story)
elapsed = time.time() - start

if result and len(result) > 0 and hasattr(result[0], 'output_text'):
    output = result[0].output_text
    if isinstance(output, bytes):
        output = output.decode('utf-8')
    
    print(f"\nâœ… æ¨ç†æˆåŠŸ")
    print("="*70)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   Input:  {len(long_story):,} å­—ç¬¦ (~{len(long_story.split())} å•è¯)")
    print(f"   Output: {len(output):,} å­—ç¬¦ (~{len(output.split())} å•è¯)")
    print(f"   Total:  {len(long_story) + len(output):,} å­—ç¬¦")
    print(f"   Time:   {elapsed:.2f}s")
    print()
    
    # è®¡ç®—ååé‡
    total_tokens = (len(long_story) + len(output)) // 4  # ç²—ç•¥ä¼°è®¡ tokens
    throughput = total_tokens / elapsed
    print(f"   ååé‡: {throughput:.2f} tokens/s")
    print()
    
    print(f"ğŸ“ Output é¢„è§ˆ (å‰ 300 å­—ç¬¦):")
    print("-"*70)
    print(output[:300])
    if len(output) > 300:
        print("...")
        print(f"(è¿˜æœ‰ {len(output) - 300} å­—ç¬¦)")
    print("-"*70)
else:
    print(f"\nâš ï¸  ç»“æœå¼‚å¸¸: {result}")

llm.stop_server()

print("\n" + "="*70)
print("âœ… 2k Context æµ‹è¯•å®Œæˆ")
print("="*70)
print()
print("ğŸ’¡ å…³é”®é…ç½®:")
print("   âœ… do_sample=False (Greedy, å…¼å®¹å¼ é‡å¹¶è¡Œ)")
print("   âœ… tensor_parallelism_degree=4 (4å¡å¹¶è¡Œ)")
print("   âœ… max_seq_length=2048 (æ”¯æŒ 2k tokens)")
print()
