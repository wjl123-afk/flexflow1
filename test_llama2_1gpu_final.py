#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FlexFlow + Llama-2-7b 1GPU åŠŸèƒ½éªŒè¯æµ‹è¯•
"""
import flexflow.serve as ff
import time

print("=" * 70)
print("ğŸ¦™ FlexFlow + Llama-2-7b åŠŸèƒ½éªŒè¯")
print("=" * 70)
print()

# åˆå§‹åŒ–
print("ã€1/6ã€‘åˆå§‹åŒ– FlexFlow...")
start = time.time()
ff.init(
    num_gpus=1,
    memory_per_gpu=14000,
    zero_copy_memory_per_node=40000,
    tensor_parallelism_degree=1,
    pipeline_parallelism_degree=1
)
print(f"âœ… å®Œæˆ (è€—æ—¶: {time.time()-start:.2f}s)")
print()

# åŠ è½½æ¨¡å‹
print("ã€2/6ã€‘åŠ è½½æ¨¡å‹...")
start = time.time()
model_path = "/root/.cache/huggingface/hub/models--meta-llama--llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9"
llm = ff.LLM(model_path)
print(f"âœ… å®Œæˆ (è€—æ—¶: {time.time()-start:.2f}s)")
print()

# é…ç½®
print("ã€3/6ã€‘é…ç½®ç”Ÿæˆå‚æ•°...")
generation_config = ff.GenerationConfig(
    do_sample=False,
    temperature=0.9,
    topp=0.8,
    topk=1
)
print("âœ… å®Œæˆ")
print()

# ç¼–è¯‘
print("ã€4/6ã€‘ç¼–è¯‘æ¨¡å‹...")
start = time.time()
llm.compile(
    generation_config,
    max_requests_per_batch=1,
    max_seq_length=256,
    max_tokens_per_batch=128
)
print(f"âœ… å®Œæˆ (è€—æ—¶: {time.time()-start:.2f}s)")
print()

# å¯åŠ¨æœåŠ¡å™¨
print("ã€5/6ã€‘å¯åŠ¨æœåŠ¡å™¨...")
start = time.time()
llm.start_server()
print(f"âœ… å®Œæˆ (è€—æ—¶: {time.time()-start:.2f}s)")
print()

# æ¨ç†æµ‹è¯•
print("ã€6/6ã€‘æ¨ç†æµ‹è¯•...")
print("=" * 70)
prompt = "Hello, my name is"
print(f"ğŸ“ Prompt: {prompt}")
print()

start = time.time()
result = llm.generate(prompt)
inference_time = time.time() - start

print(f"â±ï¸  æ¨ç†è€—æ—¶: {inference_time:.2f}s")
print()

# è§£æç»“æœ
if isinstance(result, list) and len(result) > 0:
    print(f"ğŸ” Result ç±»å‹: {type(result[0])}")
    print(f"ğŸ” Result å¯¹è±¡: {result[0]}")
    print()
    
    if hasattr(result[0], 'output_text'):
        output_text = result[0].output_text
        if isinstance(output_text, bytes):
            output_text = output_text.decode('utf-8')
        
        print(f"âœ… ç”ŸæˆæˆåŠŸï¼")
        print()
        print(f"ğŸ¤– ç”Ÿæˆå†…å®¹:")
        print("-" * 70)
        print(output_text)
        print("-" * 70)
        print()
        print(f"ğŸ“Š ç»Ÿè®¡:")
        print(f"   - è¾“å‡ºé•¿åº¦: {len(output_text)} å­—ç¬¦")
        print(f"   - è¾“å‡ºè¯æ•°: ~{len(output_text.split())} è¯")
        print(f"   - å¹³å‡é€Ÿåº¦: ~{len(output_text.split())/inference_time:.2f} tokens/s")
    else:
        print("âš ï¸  æ— æ³•è·å– output_text å±æ€§")
        print(f"å¯ç”¨å±æ€§: {dir(result[0])}")
else:
    print(f"âŒ ç”Ÿæˆå¤±è´¥: {result}")

print()
print("=" * 70)

# åœæ­¢æœåŠ¡å™¨
print("ğŸ›‘ åœæ­¢æœåŠ¡å™¨...")
llm.stop_server()
print("âœ… å·²åœæ­¢")
print()

# æ€»ç»“
print("=" * 70)
print("ğŸ‰ FlexFlow + Llama-2-7b åŠŸèƒ½éªŒè¯å®Œæˆï¼")
print("=" * 70)
print()
print("éªŒè¯ç»“æœ:")
print("  âœ… åˆå§‹åŒ–")
print("  âœ… æ¨¡å‹åŠ è½½")
print("  âœ… æ¨¡å‹ç¼–è¯‘")
print("  âœ… æœåŠ¡å™¨å¯åŠ¨")
print("  âœ… æ¨ç†ç”Ÿæˆ")
print("  âœ… æœåŠ¡å™¨åœæ­¢")
print()
print("é…ç½®: 1x GPU | Llama-2-7b | FP16")
print("=" * 70)
