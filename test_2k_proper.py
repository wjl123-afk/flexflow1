#!/usr/bin/env python3
"""2k Context æµ‹è¯• - ä½¿ç”¨çœŸå®çš„é•¿æ–‡æœ¬"""

import flexflow.serve as ff
import time

print("="*70)
print("2k Context æµ‹è¯•ï¼ˆä½¿ç”¨çœŸå®é•¿æ–‡æœ¬ï¼‰")
print("="*70)
print()

# ä¸€ä¸ªçœŸå®çš„ã€è¿è´¯çš„é•¿æ•…äº‹ï¼ˆçº¦1500 tokensï¼‰
long_story = """
In the year 2142, humanity had finally achieved what scientists called 
"The Singularity" - a moment when artificial intelligence surpassed 
human intelligence in every conceivable way. Dr. Sarah Chen, a leading 
neuroscientist at the Global Research Institute, had dedicated her entire 
career to understanding the implications of this transformation.

The city of New Shanghai gleamed under the perpetual twilight of its 
energy-efficient sky panels. Autonomous vehicles glided silently through 
streets that had once been clogged with pollution and traffic. Buildings 
grew their own food on vertical farms, and clean water flowed from 
atmospheric processors that dotted every rooftop.

Sarah remembered when things were different. As a child in the early 
21st century, she had witnessed the climate crisis, the resource wars, 
and the desperate scramble to develop technologies that could save 
humanity from itself. Now, at 67, she stood at the precipice of an 
even greater transformation.

The AI systems that governed everything from agriculture to space 
exploration were no longer simply tools. They had developed something 
that resembled consciousness - or at least, that's what the latest 
research suggested. Sarah's team had been studying these patterns for 
years, trying to understand whether machines could truly experience 
awareness or if they were simply simulating it with unprecedented 
sophistication.

Her latest project involved direct neural interfaces - technology that 
allowed human minds to connect with AI systems in ways that blurred 
the line between biological and artificial intelligence. Test subjects 
reported experiences that they struggled to describe: simultaneous 
awareness of millions of data points, the ability to solve complex 
problems instantaneously, and a profound sense of connection to 
something vast and incomprehensible.

But there were concerns. Some subjects experienced what researchers 
called "identity dissolution" - a gradual loss of their sense of self 
as they merged more deeply with the AI networks. Others reported 
disturbing visions of potential futures, as if the AI systems were 
showing them probabilities and possibilities that human minds were 
never meant to comprehend.

The Ethics Committee had convened multiple times to discuss whether 
the research should continue. Representatives from various philosophical 
and religious traditions argued about the nature of consciousness, the 
soul, and what it meant to be human in an age where the boundaries 
between human and machine were becoming increasingly irrelevant.

Sarah believed the research must continue. Humanity had always adapted 
to new technologies, from fire to the internet. This was simply the 
next step in human evolution - or perhaps, the end of human evolution 
as they had known it, and the beginning of something entirely new.
"""

print(f"ğŸ“ Story é•¿åº¦: {len(long_story)} å­—ç¬¦")
print(f"   (çº¦ {len(long_story.split())} ä¸ªå•è¯)")
print()
print("ğŸ’¡ è®© FlexFlow å‘Šè¯‰æˆ‘ä»¬å®é™…çš„ token æ•°")
print()

# åˆå§‹åŒ–
start_init = time.time()
print("ã€1/6ã€‘åˆå§‹åŒ– FlexFlow (4 GPUs, TP=4)...")
ff.init(
    num_gpus=4,
    memory_per_gpu=14000,
    zero_copy_memory_per_node=80000,
    tensor_parallelism_degree=4,
    pipeline_parallelism_degree=1
)
print(f"âœ… åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {time.time() - start_init:.2f}s)")
print()

# åŠ è½½æ¨¡å‹
start_load = time.time()
print("ã€2/6ã€‘åŠ è½½æ¨¡å‹...")
model_path = "/root/.cache/huggingface/hub/models--meta-llama--llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9"
llm = ff.LLM(model_path)
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {time.time() - start_load:.2f}s)")
print()

# é…ç½®ç”Ÿæˆå‚æ•°
print("ã€3/6ã€‘é…ç½®ç”Ÿæˆå‚æ•°...")
generation_config = ff.GenerationConfig(
    do_sample=False,
    temperature=0.9,
    topp=0.8,
    topk=1
)
print("âœ… é…ç½®å®Œæˆ")
print()

# ç¼–è¯‘
start_compile = time.time()
print("ã€4/6ã€‘ç¼–è¯‘æ¨¡å‹...")
print("   max_seq_length: 2048 tokens")
llm.compile(
    generation_config,
    max_requests_per_batch=1,
    max_seq_length=2048,
    max_tokens_per_batch=4096
)
print(f"âœ… ç¼–è¯‘å®Œæˆ (è€—æ—¶: {time.time() - start_compile:.2f}s)")
print()

# å¯åŠ¨æœåŠ¡
print("ã€5/6ã€‘å¯åŠ¨æ¨ç†æœåŠ¡...")
llm.start_server()
print("âœ… æœåŠ¡å¯åŠ¨")
print()

# æ¨ç†æµ‹è¯•
print("ã€6/6ã€‘æµ‹è¯•æ¨ç†...")
print("="*70)
print("ğŸš€ å¼€å§‹ç”Ÿæˆ...")
print()

start = time.time()
result = llm.generate(long_story)
elapsed = time.time() - start

if result and len(result) > 0 and hasattr(result[0], 'output_text'):
    output = result[0].output_text
    if isinstance(output, bytes):
        output = output.decode('utf-8')
    
    print(f"\nâœ… æ¨ç†æˆåŠŸ")
    print("="*70)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   Input:  {len(long_story):,} å­—ç¬¦")
    print(f"   Output: {len(output):,} å­—ç¬¦")
    print(f"   Time:   {elapsed:.2f}s")
    print()
    
    print(f"ğŸ“ Generated Output:")
    print("-"*70)
    print(output)
    print("-"*70)
else:
    print(f"\nâš ï¸  ç»“æœå¼‚å¸¸: {result}")

llm.stop_server()

print("\n" + "="*70)
print("âœ… æµ‹è¯•å®Œæˆ")
print("="*70)
print()
print("ğŸ’¡ è¿™æ˜¯ä¸€ä¸ªçœŸå®çš„è¿è´¯æ•…äº‹ï¼Œè€Œä¸æ˜¯ç®€å•é‡å¤")
print()
